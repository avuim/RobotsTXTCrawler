# Robots.txt for crawler

User-agent: *

# Disallow Crawler
Disallow: /User
Disallow: /Dateien
Disallow: /Nachrichten/Suche
# Crawler often creates invalid script/webresource resource request
Disallow: /ScriptResource
Disallow: /WebResource

# Max crawler Time per page in sec
Crawl-Delay: 2

#Sitemap
#Sitemap: https://www.gaeubote.de/Sitemap_Index.xml.gz