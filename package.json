{
  "name": "robots-txt-crawler",
  "version": "1.0.0",
  "description": "Modern webcrawler to extract robots.txt files from websites",
  "main": "dist/src/app.js",
  "scripts": {
    "build": "tsc",
    "start": "node dist/src/app.js",
    "dev": "ts-node src/app.ts",
    "test": "jest",
    "lint": "eslint . --ext .ts"
  },
  "keywords": [
    "robots.txt",
    "crawler",
    "web",
    "playwright",
    "axios"
  ],
  "author": "",
  "license": "ISC",
  "dependencies": {
    "@types/uuid": "^10.0.0",
    "axios": "^1.6.0",
    "cli-progress": "^3.12.0",
    "cors": "^2.8.5",
    "express": "^5.1.0",
    "fs-extra": "^11.1.1",
    "p-limit": "^3.1.0",
    "p-retry": "^5.1.2",
    "playwright": "^1.40.0",
    "uuid": "^11.1.0",
    "winston": "^3.11.0",
    "zod": "^3.22.4"
  },
  "devDependencies": {
    "@types/cli-progress": "^3.11.6",
    "@types/cors": "^2.8.19",
    "@types/express": "^5.0.3",
    "@types/fs-extra": "^11.0.4",
    "@types/jest": "^29.5.6",
    "@types/node": "^20.8.10",
    "@typescript-eslint/eslint-plugin": "^6.9.1",
    "@typescript-eslint/parser": "^6.9.1",
    "eslint": "^8.52.0",
    "jest": "^29.7.0",
    "ts-jest": "^29.1.1",
    "ts-node": "^10.9.1",
    "typescript": "^5.2.2"
  }
}
